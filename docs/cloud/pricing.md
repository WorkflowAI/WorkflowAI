# Pricing

WorkflowAI Cloud uses a pay-as-you-go infrastructure, similar to Amazon Web Services. There is no fixed cost, minimum spend, annual commitment, and no need to talk to sales to get started.

## Our Price match guarantee

We offer a price-match guarantee for all LLM providers: WorkflowAI Cloud **charges the same per token price** as using providers directly. Currently, we support models from OpenAI, Anthropic, Google, Mistral, DeepSeek, Grok, and Llama (provided by [FireworksAI](https://fireworks.ai/)).

If you have credits with Amazon, Google, or Azure, you can also continue to use them via WorkflowAI Cloud, [by providing your own API keys](/docs/features/deployments.md#using-your-own-ai-providers-api).

## What does WorkflowAI Cloud charge for?
WorkflowAI Cloud **only** charges for:
- the tokens generated by your AI features (per token generated)
- the tools used by your AI features (per tool used)

We **do not** charge for:
- data storage
- quantity of AI features
- users in your organization
- bandwidth or CPU usage

## Then how does WorkflowAI Cloud make money?

We make our margin by buying LLM tokens at bulk discount, and then reselling them to you at the standard public price.

To break this down further, let’s look at what actually drives the cost of inference:
The cost of inference is mostly GPU and electricity, not tokens. When you buy tokens from an LLM provider you are effectively paying for the electricity and GPU cost. But paying for GPU usage is most efficient if you can utilize the GPU at maximum capacity, all the time. That’s hard to do on your own, but possible for WorkflowAI because we’re pooling demand from our users. By taking this approach of pooling demand, WorkflowAI can keep your LLM costs low while still giving you access to a wide range of providers.