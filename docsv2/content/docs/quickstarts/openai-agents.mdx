---
title: OpenAI Agents
description: How to use the OpenAI Agents SDK with WorkflowAI
---

## Setup

Works. The example below is not the best use-case to showcase, will need to do a better one.

- tracing needs to be disabled, at least for now. (until we have OTEL)

```python
"""
Example: Using WorkflowAI proxy with the OpenAI Agents SDK
--------------------------------------------------------
This demo shows how to:
1. Configure the Agents SDK to call the WorkflowAI proxy (OpenAI‑compatible).
2. Build two specialist agents (Translator & Summarizer).
3. Add a Router agent that decides when to hand‑off.
Running the file will trigger a hand‑off and print the final answer.

Prerequisites:
    pip install openai openai-agents
"""

import asyncio
from openai import AsyncOpenAI
from agents import (
    Agent,
    Runner,
    set_default_openai_client,
    set_default_openai_api,
    set_tracing_disabled,
)
from agents import RunConfig, ModelSettings
# ── 1. Configure the SDK to use WorkflowAI ──────────────────────────────────────
workflowai_client = AsyncOpenAI(
    # NOTE: keep the trailing slash – the SDK expects it.
    base_url="https://run.workflowai.dev/v1/",
    api_key="wai-***",
)

# Make every model call go through the WorkflowAI proxy
set_default_openai_client(workflowai_client)
# The proxy currently supports the Chat Completions API
set_default_openai_api("chat_completions")

set_tracing_disabled(True)

# ── 2. Specialist agents ────────────────────────────────────────────────────────
translation_agent = Agent(
    name="Translator",
    instructions=(
        "You are a professional translator. "
        "Translate the user's input from English to French. "
        "Respond ONLY with the translation; no extra commentary."
    ),
    model="gpt-4o-mini",  # any chat‑completions model exposed by WorkflowAI
)

summarizer_agent = Agent(
    name="Summarizer",
    instructions=(
        "You are an expert summarizer. "
        "Return a concise, two‑sentence summary of the user's input in English."
    ),
    model="gpt-4o-mini",
)

# ── 3. Router agent with hand‑offs ──────────────────────────────────────────────
triage_agent = Agent(
    name="Router",
    instructions=(
        "You are a router. Decide whether the user is asking to TRANSLATE text "
        "or to SUMMARIZE text.\n\n"
        "• If translation: hand‑off to Translator.\n"
        "• If summary: hand‑off to Summarizer.\n"
        "• Otherwise: answer yourself."
    ),
    handoffs=[translation_agent, summarizer_agent],
    model="o3-latest-medium",
)

# ── 4. Run an example that triggers a hand‑off ──────────────────────────────────
async def main() -> None:
    user_input = "Translate this to French: Hello, how are you today?"
    result = await Runner.run(triage_agent, input=user_input)
    print("Final answer:", result.final_output)
    # Uncomment below to inspect the call chain
    # for step in result.steps:
    #     print(step.agent_name, step.output)

if __name__ == "__main__":
    asyncio.run(main())
```

## Identify your agent

<Callout type="info">
  **Alternative Method**: This example shows how to identify your agent using the model parameter prefix. This is useful when working with frameworks that don't support custom metadata. The preferred method is to use `agent_id` in the metadata field when possible. Learn more about [both methods](/docs/observability#identify-your-agent).
</Callout>

```python
translation_agent = Agent(
    name="Translator",
    instructions=(
        "You are a professional translator. "
        "Translate the user's input from English to French. "
        "Respond ONLY with the translation; no extra commentary."
    ),
    model="translator/gpt-4o-mini",
)
```

## Accessing 100+ models

```python
translation_agent = Agent(
    ...
    model="translator/...",
)
```

## Compare models side-by-side

Would require a way to mock tool calls results?
Replay from the first message?
Model from what agent are we talking to?

## Observability

TODO: Requires OTEL.

## Input Variables

...
- Only works at the initial agent?

## Deployments

TBD.

## Evals