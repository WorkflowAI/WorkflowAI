# For Software Engineers

> “Being provider-agnostic used to mean maintaining multiple complex integrations. With WorkflowAI, we can seamlessly switch between LLM providers without any extra integration effort or overhead—saving us engineering time and headaches.” 
>
> ~ Aymeric Beaumet, CTO at M1

## Why Software Engineers like WorkflowAI:

### Our [Python SDK](../sdk/python/get-started.md)
Stop wasting time maintaining separate integrations for every LLM. WorkflowAI gives you unified, seamless access to all models through a single, clean API.

### Structured outputs
WorkflowAI ensures your AI responses always match your defined structure, simplifying integrations, reducing parsing errors, and making your data reliable and ready for use.

### Write code only when you want to
WorkflowAI gives you flexibility: quickly prototype new AI features via our intuitive web interface, or dive directly into code whenever you need deeper customization and control.

### Proudly [open-source](https://github.com/WorkflowAI/workflowai)
WorkflowAI is fully open-source with flexible deployment options. Run it self-hosted on your own infrastructure for maximum data control, or use the managed [WorkflowAI Cloud](../cloud/introduction.md) service for hassle-free updates and automatic scaling.

### Model-agnostic
Works with all major AI models including OpenAI, Anthropic, Claude, Google/Gemini, Mistral, DeepSeek, Grok with a unified interface that makes switching between providers seamless.

### [Streaming supported](../features/code#streaming)
Enables real-time streaming of AI responses for low latency applications, with immediate validation of partial outputs

## How to get started:

Read our [AI Features Playbook](../playbook/introduction.md) to learn how to build your first AI feature in minutes in our web-app, or learn more about our [Python SDK](../sdk/python/get-started.md) to build features programmatically.