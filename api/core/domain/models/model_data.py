from datetime import date
from typing import Any, Literal, TypeAlias

from pydantic import BaseModel, Field

from core.domain.errors import ProviderDoesNotSupportModelError
from core.domain.models import Model, Provider
from core.domain.task_typology import TaskTypology

from ._sourced_base_model import SourcedBaseModel
from .model_data_supports import ModelDataSupports
from .model_provider_data import ModelProviderData

# Weights were computed by o3 by comparing the median scores of models
# on the different score datasets
# https://chatgpt.com/share/681cb51b-6aa4-800e-b966-b6b8a4e58021
_quality_index_weights = {
    "mmlu": 0.15,
    "gpqa": 0.25,
    "mmlu_pro": 0.25,
    "gpqa_diamond": 0.35,
}


class QualityData(SourcedBaseModel):
    mmlu: float | None = None
    gpqa: float | None = None
    mmlu_pro: float | None = None
    gpqa_diamond: float | None = None
    index: int | None = Field(
        default=None,
        description="A forced value for the quality index",
    )

    equivalent_to: tuple[Model, int] | None = Field(
        default=None,
        description="When data is not available for a model, we can use the quality index of an equivalent model",
    )

    # TODO: remove the default value
    source: str = ""

    def _quality_index_from_equivalent_model(self, mapping: dict[Model, Any]) -> int:
        if not self.equivalent_to:
            raise ValueError("Equivalent model is none")

        model, offset = self.equivalent_to

        raw_data = mapping[model]
        if not isinstance(raw_data, ModelData):
            raise ValueError(f"Equivalent model {model} is not a ModelData")
        if raw_data.quality_data.equivalent_to is not None:
            raise ValueError(f"Equivalent model {model} has an equivalent model of its own")
        return raw_data.quality_data.quality_index(mapping) + offset

    def quality_index(self, mapping: dict[Model, Any]) -> int:
        if self.index is not None:
            return self.index
        if self.equivalent_to:
            return self._quality_index_from_equivalent_model(mapping)

        # We do a weighed sum of the different scores
        dumped = self.model_dump(exclude_none=True, exclude={"source"})

        total_score: float = 0
        for k, v in dumped.items():
            total_score += v * _quality_index_weights[k]
        return int((total_score / len(dumped)) * 10)


class MaxTokensData(SourcedBaseModel):
    max_tokens: int = Field(
        description="The maximum number of tokens (input + output) that can be handled by the model.",
        gt=0,
    )
    max_output_tokens: int | None = Field(
        default=None,
        description="The maximum number of tokens that can be generated by the model.",
        gt=0,
    )


class ModelData(ModelDataSupports):
    display_name: str = Field(description="The display name of the model, that will be used in the UIs, etc.")
    icon_url: str = Field(description="The icon url of the model")

    max_tokens_data: MaxTokensData

    provider_for_pricing: Provider

    latest_model: Model | None = Field(
        default=None,
        description="The latest model for the family",
    )

    is_default: bool = Field(
        default=False,
        description="If true, the model will be used as default model.",
    )

    release_date: date = Field(description="The date the model was released")

    quality_data: QualityData = Field(
        description="The quality data of the model which allows computing the quality index",
    )

    provider_name: str = Field(
        description="The name of the provider for the model",
    )

    supports_tool_calling: bool = Field(
        description="Whether the model supports tool calling",
    )

    # TODO: most thinking models don't use that value yet
    # Use none for models to deactivate reasoning on thinking models
    reasoning_level: Literal["none", "low", "medium", "high"] | None = None

    @property
    def modes(self) -> list[str]:
        out: list[str] = []
        if self.supports_json_mode:
            out.append("text")

        if self.supports_input_image:
            out.append("images")

        if self.supports_input_audio:
            out.append("audio")
        return out


class FinalModelData(ModelData):
    model: Model

    # TODO: this should be a dict since it's ordered
    providers: list[tuple[Provider, ModelProviderData]] = Field(
        description="The provider data for the model. Extracted from the model provider data list",
    )

    quality_index: int

    def supported_by_provider(self, provider: Provider) -> bool:
        return any(p == provider for p, _ in self.providers)

    def provider_data(self, provider: Provider) -> ModelProviderData:
        for p, provider_data in self.providers:
            if p == provider:
                return provider_data
        raise ProviderDoesNotSupportModelError(self.model, provider)

    def provider_data_for_pricing(self) -> ModelProviderData:
        """Returns the provider data for the model for pricing purposes"""
        # The for loop is not the best solution, but we will at most have 2 providers
        # and usually the provider that is used is the first one in the array since they are
        # ordered by priority
        for provider, provider_data in self.providers:
            if provider == self.provider_for_pricing:
                return provider_data
        # This should never happen, we have tests for that
        raise ValueError(f"Provider {self.provider_for_pricing} not found for model {self.display_name}")

    def is_not_supported_reason(  # noqa: C901
        self,
        task_typology: TaskTypology,
    ) -> str | None:
        """Returns the reason why the model is not supported for the given task typology or
        None if the task typology is supported"""

        # Fireworks supports document inlining which makes models without vision "support" vision
        # TODO[models]: Having this explicit exception is not great. Instead we should
        # make the model data represent what WorkflowAI support instead of the model card
        supports_inlining = any(provider == Provider.FIREWORKS for provider, _ in self.providers)

        if self.supports_audio_only and not task_typology.input.has_audio:
            return f"{self.display_name} does not support non-audio inputs"
        if task_typology.input.has_image and not self.supports_input_image:
            if supports_inlining:
                return None
            return f"{self.display_name} does not support input images"
        if task_typology.input.has_audio and not self.supports_input_audio:
            return f"{self.display_name} does not support input audio"
        if task_typology.input.has_pdf and (not self.supports_input_pdf and not self.supports_input_image):
            if supports_inlining:
                return None
            return f"{self.display_name} does not support input pdf"

        if task_typology.output.has_image and not self.supports_output_image:
            return f"{self.display_name} does not support output images"
        # Right now we have no model supporting output audio or PDF but that could change in the future
        if task_typology.output.has_audio:
            return f"{self.display_name} does not support output audio"
        if task_typology.output.has_pdf:
            return f"{self.display_name} does not support output pdf"

        if task_typology.output.has_text and not self.supports_output_text:
            return f"{self.display_name} only supports data outputs"
        return None


class LatestModel(BaseModel):
    # Used to map a latest model to a specific model
    model: Model
    display_name: str
    is_default: bool = False
    icon_url: str = ""  # Fixed at build time


class DeprecatedModel(BaseModel):
    replacement_model: Model


ModelDataMapping: TypeAlias = dict[Model, FinalModelData | LatestModel | DeprecatedModel]
