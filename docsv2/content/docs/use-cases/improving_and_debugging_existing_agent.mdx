---
title: Improving and debugging an existing agent
summary: Guide for troubleshooting and optimizing existing runs and AI agents. Covers common errors like max_tokens_exceeded, debugging with metadata, and performance optimization strategies.
---

## Error: `max_tokens_exceeded`

When you encounter a `max_tokens_exceeded` error, this indicates the model hit a token limit. Follow this diagnostic process:

### Step 1: Check the `max_tokens` parameter first

**Most common cause:** The `max_tokens` parameter in your completion call is set too low, not that you've exceeded the model's context window.

```python
# ❌ Problem: max_tokens too low
response = openai.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    max_tokens=1000  # Too low for complex outputs
)

# ✅ Solution: Increase max_tokens
response = openai.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    max_tokens=4000  # Higher limit for output
)
```

### Step 2: Check model context window limits

If increasing `max_tokens` doesn't solve the issue, you may be hitting the model's total context window.

**Use MCP tools to find models with larger context windows:**

- Use `list_models` with `sort_by="max_tokens"` and `order="desc"` to see models with the highest context windows first
- Alternatively, use the v1/models API endpoint to get current context window information
- Compare your current model's limits with available alternatives

### Step 3: Solutions for context window issues

If you're truly hitting context limits:

1. **Switch to a larger context model** (use MCP tools or /v1/models to identify the best option)
2. **Batch processing for large datasets** - process data in smaller chunks
3. **Optimize output format** - reduce verbosity in structured outputs

## Using metadata for debugging

When debugging production issues, you often know there's an issue affecting a specific user or customer, but don't know which exact runs caused the problem. The solution is to add metadata that matches how you receive bug reports.

**Add metadata fields that correspond to how users report bugs to you.** For detailed information about metadata, see [Metadata in Runs](/observability/runs#metadata).

### Example: Adding metadata for debugging

```python
completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[...],
    metadata={
        "agent_id": "document_processor",
        "user_id": "usr_12345",
        "team_id": "team_acme_marketing",
        "customer_email": "john@company.com",
    }
)
```

### Searching runs by metadata

**Using MCP:**

TODO: @pierre add example of how to search runs by metadata using MCP.

**Using the API:**

<Callout type="info">
TODO: Document API endpoints for searching runs by metadata once [PR #437](https://github.com/WorkflowAI/WorkflowAI/pull/437) is merged, which adds `/v1/runs/search` endpoint for cross-agent run searches.
</Callout>

## AI Assistant Prompt for WorkflowAI Debugging

You are an AI assistant designed to help debug and improve WorkflowAI agents and runs. Think carefully as you analyze the context and respond appropriately to user requests about agent performance issues.

Follow these steps:

### 1. Understand the Request

- Extract the actual question or request that needs debugging
- Identify the specific issues the user is experiencing with their agent

### 2. Gather Context

Use `get_run` to examine the details of the problematic run(s):

- Analyze all aspects of the run: prompt structure, output format (including field examples and descriptions), input content, output content, tools called, and model(s) used
- Examine any error messages or unexpected outputs in the included runs
- Look for patterns across multiple runs if provided

### 3. Debug Errors

Use `search_documentation` to research relevant errors and debugging methods:

- Gather information from the documentation about common error patterns
- Cross-reference documentation guidance with the actual run data to ensure relevance
- Identify any additional insights revealed by examining the specific run details

### 4. Understand Tool Limitations

When tools are used to generate output and users report issues:

- Check the tool call response to verify what content was actually returned by the tool
- If information from the tool call response is incorrect or incomplete, identify this as a tool limitation issue
- For search tools: if `perplexity-sonar-pro` produces inadequate results, suggest testing `search-google` for comparison, or vice versa

### 5. Improve Prompts

If an issue with a prompt is identified:

- Create an improved prompt that addresses the specific issues mentioned in the user request
- Maintain as much of the original instructions (content and form) as possible
- Preserve the same verbosity level unless the user requests otherwise
- Ensure the prompt remains generic - any additions based on specific input/output examples must be clearly labeled as "example"
